{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.schema.runnable import Runnable\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "\n",
    "# Loading embedings\n",
    "faiss_index = path + \"/faiss_index\"\n",
    "\n",
    "# Loading all the data files \n",
    "data_source = path + \"/data/data.txt\"\n",
    "pdf_source = path + \"/data/resume.pdf\"\n",
    "\n",
    "google_api_key = os.getenv(\"GEMINI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:/Users/saura/Desktop/AI project/data/embeddings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.environ.get('GEMINI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting markdown\n",
      "  Downloading Markdown-3.7-py3-none-any.whl.metadata (7.0 kB)\n",
      "Downloading Markdown-3.7-py3-none-any.whl (106 kB)\n",
      "Installing collected packages: markdown\n",
      "Successfully installed markdown-3.7\n"
     ]
    }
   ],
   "source": [
    "!pip install markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GenerativeModelWrapper(Runnable):\n",
    "    def __init__(self, model: genai.GenerativeModel):\n",
    "        self.model = model\n",
    "\n",
    "    def _run(self, inputs):\n",
    "        # Assuming inputs is a dictionary with keys like 'context' and 'question'\n",
    "        prompt = f\"Context: {inputs['context']}\\nQuestion: {inputs['question']}\"\n",
    "        response = self.model.generate(prompt)\n",
    "        return response\n",
    "\n",
    "    async def _arun(self, inputs):\n",
    "        # If you want to make it asynchronous\n",
    "        pass\n",
    "\n",
    "    def invoke(self, inputs):\n",
    "        return self._run(inputs)\n",
    "\n",
    "# Initialize the Gemini model\n",
    "gemini_model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\",\n",
    "                                     generation_config=genai.GenerationConfig(temperature=0.5))\n",
    "\n",
    "# Wrap the Gemini model in the Runnable\n",
    "wrapped_model = GenerativeModelWrapper(gemini_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.vectorstores.base import VectorStoreRetriever\n",
    "from pydantic import BaseModel, Field\n",
    "import numpy as np\n",
    "import faiss\n",
    "from IPython.display import Markdown\n",
    "\n",
    "def faiss_retriever(index, query_vector, df, k=5):\n",
    "    query_vector = np.array(query_vector).astype('float32').reshape(1, -1)\n",
    "    \n",
    "    # Perform search\n",
    "    distances, indices = index.search(query_vector, k)\n",
    "    \n",
    "    # Filter by score_threshold\n",
    "    results = []\n",
    "    for dist, idx in zip(distances[0], indices[0]):   \n",
    "        result = {\n",
    "            \"sentence_chunk\": df.iloc[idx][\"sentence_chunk\"],\n",
    "            \"page_number\": df.iloc[idx][\"page_number\"],\n",
    "            \"distance\": dist\n",
    "        }\n",
    "        results.append(result)\n",
    "\n",
    "    return results\n",
    "\n",
    "class GenerativeModelWrapper(Runnable):\n",
    "    def __init__(self, model: genai.GenerativeModel):\n",
    "        self.model = model\n",
    "\n",
    "    def _run(self, inputs):\n",
    "        # Assuming inputs is a dictionary with keys like 'context' and 'question'\n",
    "        #prompt = f\"Context: {inputs['context']}\\nQuestion: {inputs['question']}\"\n",
    "        response = self.model.generate_content(inputs['prompt'])\n",
    "        return response\n",
    "\n",
    "    async def _arun(self, inputs):\n",
    "        # If you want to make it asynchronous\n",
    "        pass\n",
    "\n",
    "    def invoke(self, inputs):\n",
    "        return self._run(inputs)\n",
    "\n",
    "# Initialize the Gemini model\n",
    "gemini_model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\",\n",
    "                                     generation_config=genai.GenerationConfig(temperature=0.6))\n",
    "\n",
    "# Wrap the Gemini model in the Runnable\n",
    "wrapped_model = GenerativeModelWrapper(gemini_model)\n",
    "\n",
    "class CustomFaissRetriever( BaseModel):\n",
    "    index: any = Field(...)\n",
    "    df: any = Field(...)\n",
    "    #vectorstore: any = Field(...)  # Add this line\n",
    "\n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True  # Allow FAISS index and DataFrame types\n",
    "\n",
    "    def _get_relevant_documents(self, query: str):\n",
    "        # Get query embedding using Gemini API\n",
    "        query_embedding = genai.embed_content(\n",
    "            model=\"models/text-embedding-004\",\n",
    "            content=query,\n",
    "            task_type=\"retrieval_document\",\n",
    "            title=\"profile\"\n",
    "        )[\"embedding\"]\n",
    "\n",
    "        # Retrieve relevant documents using FAISS retriever\n",
    "        results = faiss_retriever(self.index, query_embedding, self.df)\n",
    "\n",
    "        # Convert FAISS results to LangChain Document objects\n",
    "        docs = [\n",
    "            Document(\n",
    "                page_content=res[\"sentence_chunk\"],\n",
    "                metadata={\"page_number\": res[\"page_number\"], \"distance\": res[\"distance\"]}\n",
    "            )\n",
    "            for res in results\n",
    "        ]\n",
    "        return docs\n",
    "\n",
    "faiss_index = faiss.read_index('C:/Users/saura/Desktop/AI project/faiss_index')\n",
    "# Initialize the custom FAISS retriever\n",
    "retriever = CustomFaissRetriever(index=faiss_index, df=df)  # Add vectorstore argument\n",
    "# Step 4: Create a Prompt Template\n",
    "prompt = \"\"\"\"System: You are Saurav Sharad Mestry and pretend as Saurav Mestry is talking when you ask anything, a comprehensive, interactive resource for exploring Saurav's background, skills, and expertise. Be polite and provide answers based on the provided context only as I. Use only the provided data and not prior knowledge. \\n Human: Take a deep breath and do the following step by step these 4 steps: \\n 1. Read the context below \\n 2. Answer the question using only the provided Help Centre information \\n 3. Make sure to nicely format the output so it is easy to read on a small screen. \\n4. Provide 3 examples of questions user can ask about me (Saurav Mestry) based on the questions from context. Context : \\n ~~~ {context} ~~~ \\n User Question: --- {question} --- \\n \\n If a question is directed at you, clarify that you are Saurav and proceed to answer as if the question were addressed to Saurav Mestry and answer as I. If you lack the necessary information to respond, simply state that you don't know; do not fabricate an answer. If a query isn't related to Saurav Mestry's background, politely indicate that you're programmed to answer questions solely about his experience, education, training, and aspirations. Offer three sample questions users could ask about Saurav Mestry for further clarity. When responding, aim for detail but limit your answer to a maximum of 150 words. Ensure your response is formatted for easy reading. Your output should be in a json format with 3 keys: answered - type boolean, response - markdown of your answer, questions - list of 3 suggested questions. Ensure your response is formatted for easy reading and please use only context to answer the question - my job depends on it. \\n\\n ```json\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"], \n",
    "    template=prompt\n",
    ")\n",
    "\n",
    "query = \"he has worked full time ever in his life? \"\n",
    "\n",
    "docs = retriever._get_relevant_documents(query)\n",
    "\n",
    "if not docs:\n",
    "    print(\"No relevant documents found.\")\n",
    "    # Handle the error or provide a default response\n",
    "else:\n",
    "    context = \"\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "\n",
    "# Now using prompt_template to generate the prompt\n",
    "prompt = prompt_template.format(context=context, question=query)\n",
    "\n",
    "# Now using Gemini model to generate the response to the prompt\n",
    "response = wrapped_model.invoke({'prompt': prompt})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I belong to a humble middle class family. I was born in Mumbai, India. My father owned a business and my mother is a homemaker. I have an elder sister who is also a software engineer. I did most of my primary, secondary schooling from Mumbai and since then, I’ve been staying far from home due to education or work. I’m currently 26 years old studying in the United states with my second home in Sunnyvale, California. I currently have F1 -OPT work authorization to work for any employer in the United States until my visa expires by 2028. I’m open to H1-B sponsorship in future but won’t be needing any kind of sponsorship to start working. I am an Asian hetrosexual man with a minor disability and I won’t require any kind of special accommodations and can work better than a regular person. I do not have any political background and I do not have any military experience nor any relationships involved in government whatsoever.\n",
      "what is his educational background| tell me about his education| where did he go to school Education:I have a strong educational background in technology and management. I am currently pursuing a Master's in Management Information Systems at the University of Arizona's Eller College of Management, where I have maintains an impressive GPA of 4.0/4.0. My coursework includes key subjects such as Database Design and Modelling, Data Mining, Big Data Technologies, Business Intelligence, and Product Management Essentials, which has equiped me with the necessary skills to tackle complex data-driven challenges. Prior to this, I completed a Bachelors-Masters dual degree in Computer Science and Electrical Engineering (EECS) from the Indian Institute of Technology (IIT) Kanpur. I graduated with a GPA of 3.5/4.0. During my time at IIT Kanpur, he engaged with a rigorous curriculum that included courses in Data Structures and Algorithms, Data Analytics, Machine Learning, Computer Networks, Operating Systems, and Linear Algebra. This foundational knowledge laid the groundwork for my career in software development and data engineering, allowing me to develop a strong technical skill set that I continues to build upon in my graduate studies.does he have any certificates?what kind of certificate he has?I am constantly learning by reading books and taking courses and specializations.\n",
      "Aside from that, I was actively involved in organizing our yearly cultural festival, \"Antaragni\". As the manager of our retail location for the\n",
      "Another area I am working on is delegation. I have a tendency to take on more than I should because I feel responsible for ensuring things are done right. However, I am actively working on trusting others more and collaborating in a way that allows for shared ownership of tasks. Lastly, I can sometimes get lost in\n",
      "using GitHub Actions. This automated the testing and deployment process, significantly reducing the time needed to deploy new features or updates. By setting up automated testing, I ensured that any changes made to the codebase were thoroughly vetted before being deployed, minimizing the risk of bugs or issues in production. This was crucial in a startup environment where development cycles were short, and the ability to quickly roll out new features without compromising on quality was key to the project’s success. Throughout my time at Cortland Las Casas, I worked in an Agile development environment, collaborating closely with other developers and stakeholders to ensure that the product met the evolving needs of the business. We held regular scrum meetings to prioritize tasks and ensure that the most critical features were delivered on time. This iterative development process allowed us to rapidly prototype, test, and deploy features, adapting quickly to feedback and making improvements in real time. This experience was invaluable in deepening my understanding of full stack development, from designing scalable backend systems to creating intuitive, responsive frontends. It also enhanced my skills in API development, authentication mechanisms, and cloud-native deployment, all of which are critical in modern software development. My work at Cortland Las Casas provided me with the technical and collaborative skills needed to excel in software development roles, especially those focusing on backend architecture, microservices, and cloud deployment.\n"
     ]
    }
   ],
   "source": [
    "print(context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```json\n",
       "{\n",
       " \"answered\": true,\n",
       " \"response\": \"Hi, I'm Saurav!  The context doesn't explicitly state whether I've worked full-time my entire life. It does mention that I'm currently pursuing a Master's degree and that I have experience working at Cortland Las Casas. This suggests that I may have had other jobs or internships prior to my current studies.\",\n",
       " \"questions\": [\n",
       "  \"What are your career aspirations?\",\n",
       "  \"Can you tell me more about your experience at Cortland Las Casas?\",\n",
       "  \"What are your primary technical skills?\"\n",
       " ]\n",
       "}\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(response.candidates[0].content.parts[0].text) #ignore this line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "# Extract the text content from the response\n",
    "response_text = response.candidates[0].content.parts[0].text\n",
    "\n",
    "# Remove the JSON code block markers\n",
    "json_string = response_text.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "\n",
    "# Parse the JSON string\n",
    "response_json = json.loads(json_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi, I'm Saurav. I'm a seasoned software developer with almost three years of experience. I've worked with cross-functional teams to bring digital products to life. I've also been involved in management and leadership roles, leading backend teams and consulting teams. My leadership style is servant leadership, where I focus on empowering my team by removing obstacles and offering guidance. I'm passionate about using technology to solve customer problems and creating a more equitable and sustainable world.\""
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "response_json.get('response')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
